# -*- coding: utf-8 -*-
"""Cyber_Attack.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wKfMTfwZpSpZTtcpnwie4Do63uhoqgnn
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
from sklearn import datasets, linear_model
from sklearn.model_selection import cross_val_score
from sklearn import model_selection
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB 
from sklearn.ensemble import RandomForestClassifier
from mlxtend.classifier import StackingClassifier
from genetic_selection import GeneticSelectionCV
from sklearn.neural_network import MLPClassifier
import numpy as np
import warnings
warnings.simplefilter('ignore')
from sklearn.model_selection import GridSearchCV
import tensorflow as tf
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
from sklearn.metrics import accuracy_score
from sklearn.metrics import zero_one_loss

train_dataset = pd.read_csv("training-set.csv")
test_dataset = pd.read_csv("testing-set.csv")
train_dataset = train_dataset.drop(
    train_dataset.columns[[0, 2, 3, 4, 43]], axis=1)
test_dataset = test_dataset.drop(
    test_dataset.columns[[0, 2, 3, 4, 43]], axis=1)
frames = [train_dataset, test_dataset]
dataset = pd.concat(frames)
dataset = dataset.sample(frac=1)
dataset = dataset.astype('float32')
#dataset = np.concatenate((train_dataset, test_dataset), axis=0)
# np.random.shuffle(dataset)
x_train = dataset.iloc[:20000, 0:-1].values
y_train = dataset.iloc[:20000, -1].values
x_test = dataset.iloc[4000:, 0:-1].values
y_test = dataset.iloc[4000:, -1].values

#https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html
clf1 = MLPClassifier(solver='adam',
                      hidden_layer_sizes=(100, 100),
                      activation='relu',
                      max_iter = 50,
                      verbose=True,
                      tol= 1e-100,
                      n_iter_no_change=1,
                      early_stopping=True,
                      warm_start=True)


clf2 =  RandomForestClassifier(random_state=1)
estimator = linear_model.LogisticRegression(solver="liblinear", multi_class="ovr")
clf3  = GeneticSelectionCV(estimator,
                            cv=5,
                            verbose=1,
                            scoring="accuracy",
                            max_features=5,
                            n_population=20,
                            crossover_proba=0.5,
                            mutation_proba=0.2,
                            n_generations=40,
                            crossover_independent_proba=0.5,
                            mutation_independent_proba=0.05,
                            tournament_size=3,
                            n_gen_no_change=3,
                            caching=True,
                            n_jobs=-1)

#http://rasbt.github.io/mlxtend/user_guide/classifier/StackingClassifier/
lr = LogisticRegression()
sclf = StackingClassifier(classifiers=[clf1 , clf2 , clf3 ], 
                          meta_classifier=lr)

print('3-fold cross validation:\n')
for clf, label in zip([ clf1, clf2, sclf], 
                      ['Multi Layers Perceptron', #from sklearn.neural_network import MLPClassifier
                       'Random Forest',
                       'Stacking Classifier']):
    history = clf.fit(x_train, y_train)
    scores = model_selection.cross_val_score(clf, x_train, y_train, 
                                              cv=50, scoring='accuracy') # cv = EPOCH
    scores_test = model_selection.cross_val_score(clf, x_test, y_test, 
                                              cv=50, scoring='accuracy')
    
    predictions = clf.predict(x_test[:4000])
    target = y_test[:4000]
    correct = 0
    wrong = 0
    for i in range( len(predictions)):
        if (predictions[i] == target[i]):
            correct += 1
        else:
            wrong += 1
        print(predictions[i] , target[i])


    print('correct:', correct)
    print('wrong:', wrong)

    acc = accuracy_score(target, predictions)
    print("Accuracy: %0.2f (+/- %0.2f) [%s]" 
#           % (acc, scores.std(), label))
  

    plt.plot(scores, color='blue',
         marker='.', linestyle='dashed', linewidth=2, markersize=12)
    plt.plot(scores_test, color='red',
      marker='.', linestyle='dashed', linewidth=2, markersize=12)
    plt.title('model accuracy')
    plt.ylabel('accuracy')
    plt.xlabel('epoch')
    plt.legend(['train', 'test'], loc='upper left')
    plt.show()

    lasso = linear_model.Lasso()
    losss = cross_val_score(lasso,  x_train, y_train, cv=50)
    losss_test = cross_val_score(lasso,  x_test, y_test, cv=50)
    plt.plot(losss, color='blue',
            marker='.', linestyle='dashed', linewidth=2, markersize=12)
    plt.plot(losss_test, color='red',
        marker='.', linestyle='dashed', linewidth=2, markersize=12)
    plt.title('model loss')
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend(['train', 'test'], loc='upper left')
    plt.show()

!pip install sklearn-genetic